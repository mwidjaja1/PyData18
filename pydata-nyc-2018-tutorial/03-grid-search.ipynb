{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelize Pipeline-GridSearch with Dask Delayed\n",
    "=================================================\n",
    "\n",
    "In this exercise we parallelize hyper-parameter selection on a Scikit-Learn pipeline.  This is an example of a non-trivial parallel algorithm that we can write down with for loops and Dask Delayed.\n",
    "\n",
    "We extend an [example taken from the Scikit-Learn documentation](http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html) that builds a pipeline to transform and train text data.  We recommend that you review that example by clicking the link above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We copy the first part of that example.  The part that sets up the data and the parameters for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (5,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unroll Pipeline-GridSearch into nested for loops\n",
    "\n",
    "Normally with Scikit-Learn you would now construct a pipeline, GridSearchCV object, and call fit.  This would use complex code within Scikit-Learn to run this on your machine.\n",
    "\n",
    "This is a common operation that people want to parallelize across a cluster.  We can do so by writing out the process explicitly as a highly nested for loop.  There is one for loop for the train/test splits and then one for loop for each parameter over which we want to iterate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_scores = []\n",
    "\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "\n",
    "    for max_df in [0.5, 0.75, 1.0]:\n",
    "        for ngram_range in [(1, 1), (1, 2)]:\n",
    "            vect = CountVectorizer(max_df=max_df, ngram_range=ngram_range)\n",
    "            vect = vect.fit(X_train)\n",
    "            X2_train = vect.transform(X_train)\n",
    "            X2_test = vect.transform(X_test)\n",
    "            for norm in ['l1', 'l2']:\n",
    "                tfidf = TfidfTransformer(norm=norm)\n",
    "                tfidf = tfidf.fit(X2_train)\n",
    "                X3_train = tfidf.transform(X2_train)\n",
    "                X3_test = tfidf.transform(X2_test)\n",
    "                \n",
    "                for max_iter in [5]:\n",
    "                    for alpha in [0.00001, 0.000001]:\n",
    "                        for penalty in ['l2', 'elasticnet']:\n",
    "                            clf = SGDClassifier(max_iter=max_iter, alpha=alpha, penalty=penalty)\n",
    "                            clf = clf.fit(X3_train, y_train)\n",
    "                            \n",
    "                            score = clf.score(X3_test, y_test)\n",
    "                            params = {\n",
    "                                'max_df': max_df,\n",
    "                                'ngram_range': ngram_range,\n",
    "                                'norm': norm,\n",
    "                                'max_iter': max_iter,\n",
    "                                'alpha': alpha,\n",
    "                                'penalty': penalty\n",
    "                            }\n",
    "                            \n",
    "                            parameter_scores.append((params, score))\n",
    "                            \n",
    "best = max(parameter_scores, \n",
    "           key=lambda param_score: param_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Parallelize the computation above with Dask Delayed\n",
    "\n",
    "1.  Use Dask delayed to parallelize the code above.  \n",
    "2.  Check your graph using the `.visualize` method or `dask.visualize` function\n",
    "3.  Start a Dask cluster using `KubeCluster`\n",
    "4.  Run your computation on the cluster.  \n",
    "5.  Is it faster or is it slower?  \n",
    "6.  Use the dashboard to determine what operations are taking up the most time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/03-grid-search.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Dask-ML\n",
    "\n",
    "Operations like these are already built and available in [Dask ML](https://ml.dask.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
