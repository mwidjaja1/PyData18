{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelize Pipeline-GridSearch with Dask Delayed\n",
    "=================================================\n",
    "\n",
    "In this exercise we parallelize hyper-parameter selection on a Scikit-Learn pipeline.  This is an example of a non-trivial parallel algorithm that we can write down with for loops and Dask Delayed.\n",
    "\n",
    "We extend an [example taken from the Scikit-Learn documentation](http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html) that builds a pipeline to transform and train text data.  We recommend that you review that example by clicking the link above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We copy the first part of that example.  The part that sets up the data and the parameters for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['alt.atheism', 'talk.religion.misc']\n",
      "857 documents\n",
      "2 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "#categories = None\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (5,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unroll Pipeline-GridSearch into nested for loops\n",
    "\n",
    "Normally with Scikit-Learn you would now construct a pipeline, GridSearchCV object, and call fit.  This would use complex code within Scikit-Learn to run this on your machine.\n",
    "\n",
    "This is a common operation that people want to parallelize across a cluster.  We can do so by writing out the process explicitly as a highly nested for loop.  There is one for loop for the train/test splits and then one for loop for each parameter over which we want to iterate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_scores = []\n",
    "\n",
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "\n",
    "    for max_df in [0.5, 0.75, 1.0]:\n",
    "        for ngram_range in [(1, 1), (1, 2)]:\n",
    "            vect = CountVectorizer(max_df=max_df, ngram_range=ngram_range)\n",
    "            vect = vect.fit(X_train)\n",
    "            X2_train = vect.transform(X_train)\n",
    "            X2_test = vect.transform(X_test)\n",
    "            for norm in ['l1', 'l2']:\n",
    "                tfidf = TfidfTransformer(norm=norm)\n",
    "                tfidf = tfidf.fit(X2_train)\n",
    "                X3_train = tfidf.transform(X2_train)\n",
    "                X3_test = tfidf.transform(X2_test)\n",
    "                \n",
    "                for max_iter in [5]:\n",
    "                    for alpha in [0.00001, 0.000001]:\n",
    "                        for penalty in ['l2', 'elasticnet']:\n",
    "                            clf = SGDClassifier(max_iter=max_iter, alpha=alpha, penalty=penalty)\n",
    "                            clf = clf.fit(X3_train, y_train)\n",
    "                            \n",
    "                            score = clf.score(X3_test, y_test)\n",
    "                            params = {\n",
    "                                'max_df': max_df,\n",
    "                                'ngram_range': ngram_range,\n",
    "                                'norm': norm,\n",
    "                                'max_iter': max_iter,\n",
    "                                'alpha': alpha,\n",
    "                                'penalty': penalty\n",
    "                            }\n",
    "                            \n",
    "                            parameter_scores.append((params, score))\n",
    "                            \n",
    "best = max(parameter_scores, \n",
    "           key=lambda param_score: param_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'max_df': 0.75,\n",
       "  'ngram_range': (1, 1),\n",
       "  'norm': 'l1',\n",
       "  'max_iter': 5,\n",
       "  'alpha': 1e-06,\n",
       "  'penalty': 'l2'},\n",
       " 0.9674418604651163)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Parallelize the computation above with Dask Delayed\n",
    "\n",
    "1.  Use Dask delayed to parallelize the code above.  \n",
    "2.  Check your graph using the `.visualize` method or `dask.visualize` function\n",
    "3.  Start a Dask cluster using `KubeCluster`\n",
    "4.  Run your computation on the cluster.  \n",
    "5.  Is it faster or is it slower?  \n",
    "6.  Use the dashboard to determine what operations are taking up the most time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.multiprocessing\n",
    "dask.config.set(scheduler='processes')\n",
    "\n",
    "from dask.distributed import Client\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4e2bd9f55439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'elasticnet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGDClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX3_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX3_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    712\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m                          sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         X, y = check_X_y(X, y, 'csr', dtype=np.float64, order=\"C\",\n\u001b[0;32m--> 546\u001b[0;31m                          accept_large_sparse=False)\n\u001b[0m\u001b[1;32m    547\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    745\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/srv/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[0;32m/srv/conda/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# %load solutions/03-grid-search.py\n",
    "import dask\n",
    "\n",
    "parameter_scores = []\n",
    "\n",
    "for i in range(4):\n",
    "    X_train, X_test, y_train, y_test = dask.delayed(train_test_split, nout=4)(data.data, data.target, pure=False)\n",
    "\n",
    "    for max_df in [0.5, 0.75, 1.0]:\n",
    "        for ngram_range in [(1, 1), (1, 2)]:\n",
    "            vect = dask.delayed(CountVectorizer)(max_df=max_df, ngram_range=ngram_range)\n",
    "            vect = vect.fit(X_train)\n",
    "            X2_train = vect.transform(X_train)\n",
    "            X2_test = vect.transform(X_test)\n",
    "            for norm in ['l1', 'l2']:\n",
    "                tfidf = dask.delayed(TfidfTransformer)(norm=norm)\n",
    "                tfidf = tfidf.fit(X2_train)\n",
    "                X3_train = tfidf.transform(X2_train)\n",
    "                X3_test = tfidf.transform(X2_test)\n",
    "\n",
    "   \n",
    "                for max_iter in [5]:\n",
    "                    for alpha in [0.00001, 0.000001]:\n",
    "                        for penalty in ['l2', 'elasticnet']:\n",
    "                            clf = SGDClassifier(max_iter=max_iter, alpha=alpha, penalty=penalty)\n",
    "                            clf = clf.fit(X3_train, y_train)\n",
    "                            \n",
    "                            score = clf.score(X3_test, y_test)\n",
    "                            params = {\n",
    "                                'max_df': max_df,\n",
    "                                'ngram_range': ngram_range,\n",
    "                                'norm': norm,\n",
    "                                'max_iter': max_iter,\n",
    "                                'alpha': alpha,\n",
    "                                'penalty': penalty\n",
    "                            }\n",
    "                            \n",
    "                            parameter_scores.append((params, score))\n",
    "                            \n",
    "best = max(parameter_scores, \n",
    "           key=lambda param_score: param_score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/03-grid-search.py\n",
    "import dask\n",
    "\n",
    "parameter_scores = []\n",
    "\n",
    "for i in range(4):\n",
    "    X_train, X_test, y_train, y_test = dask.delayed(train_test_split, nout=4)(data.data, data.target, pure=False)\n",
    "\n",
    "    for max_df in [0.5, 0.75, 1.0]:\n",
    "        for ngram_range in [(1, 1), (1, 2)]:\n",
    "            vect = dask.delayed(CountVectorizer)(max_df=max_df, ngram_range=ngram_range)\n",
    "            vect = vect.fit(X_train)\n",
    "            X2_train = vect.transform(X_train)\n",
    "            X2_test = vect.transform(X_test)\n",
    "            for norm in ['l1', 'l2']:\n",
    "                tfidf = dask.delayed(TfidfTransformer)(norm=norm)\n",
    "                tfidf = tfidf.fit(X2_train)\n",
    "                X3_train = tfidf.transform(X2_train)\n",
    "                X3_test = tfidf.transform(X2_test)\n",
    "\n",
    "                for max_iter in [5]:\n",
    "                    for alpha in [0.00001, 0.000001]:\n",
    "                        for penalty in ['l2', 'elasticnet']:\n",
    "                            clf = dask.delayed(SGDClassifier)(max_iter=max_iter, alpha=alpha, penalty=penalty)\n",
    "                            clf = clf.fit(X3_train, y_train)\n",
    "\n",
    "                            score = clf.score(X3_test, y_test)\n",
    "                            params = {\n",
    "                                'max_df': max_df,\n",
    "                                'ngram_range': ngram_range,\n",
    "                                'norm': norm,\n",
    "                                'max_iter': max_iter,\n",
    "                                'alpha': alpha,\n",
    "                                'penalty': penalty\n",
    "                            }\n",
    "\n",
    "                            parameter_scores.append((params, score))\n",
    "\n",
    "best = dask.delayed(max)(parameter_scores,\n",
    "                         key=lambda param_score: param_score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at Dask-ML\n",
    "\n",
    "Operations like these are already built and available in [Dask ML](https://ml.dask.org)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
